{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Transformer from Scratch using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal is to create a GPT model trained on Andrew Huberman's podcast transcripts and create resonable output tokens. We will be using the self-attention architecture.\n",
    "\n",
    "<img src=\"./images/transformer_architecture.png\" alt=\"architecture_image\" width=\"600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.7 (default, Jan 25 2021, 11:14:52) \\n[GCC 5.5.0 20171010]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print python version being used\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First, we will declare the variables and parameters necessary to model the data and train the model. Batch is the number of training samples processed per iteration, block is the length of sequences used. All of these parameters may be changed and optimized.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch = 32\n",
    "block = 128\n",
    "max_iters = 1000\n",
    "learning_rate = 2e-5\n",
    "eval_iters = 100 \n",
    "n_embd = 384 #number of features\n",
    "n_head = 4 \n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "tot_ep = 95\n",
    "episode_nos = range(1, tot_ep)\n",
    "\n",
    "#train/val split\n",
    "val = 0.2\n",
    "val_eps = random.sample(episode_nos, int(val * tot_ep))\n",
    "train_eps = list(set(episode_nos) - set(val_eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tokenizer(text):\n",
    "    #re to find words\n",
    "    pattern = r\"\\b\\w+\\b\"\n",
    "    \n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_episodes_word(split, words):\n",
    "    eps = train_eps if split == 'train' else val_eps\n",
    "    #setup for data read\n",
    "    path = 'transcripts//Episode-'\n",
    "    extension = '.txt'\n",
    "    \n",
    "    split_len = 0\n",
    "    \n",
    "    episodes = []\n",
    "    for ep in eps:\n",
    "        with open(path + str(ep) + extension) as f:\n",
    "            #combine the episode into one string\n",
    "            content = [line.split(' ', 1)[-1].strip().lower() for line in f.readlines()] #list of lines\n",
    "            content = re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(content)) #one string\n",
    "            \n",
    "            #add any new tokens to words\n",
    "            tokens = _tokenizer(content)\n",
    "            length = len(tokens)\n",
    "            #add length\n",
    "            split_len += length\n",
    "            \n",
    "            words = list(set(words).union(set(tokens)))\n",
    "            \n",
    "            episodes.append(content)\n",
    "    return episodes, split_len, words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_content = ' '.join(load_episodes_char(train_eps))\n",
    "#val_content = ' '.join(load_episodes_char(val_eps))\n",
    "words = []\n",
    "train_content, train_len, words = load_episodes_word('train', words)\n",
    "val_content, val_len, words = load_episodes_word('val', words)\n",
    "\n",
    "train_content = _tokenizer(' '.join(train_content))\n",
    "val_content = _tokenizer(' '.join(val_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'huberman',\n",
       " 'lab',\n",
       " 'podcast',\n",
       " 'where',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'science',\n",
       " 'and',\n",
       " 'sciencebased',\n",
       " 'tools',\n",
       " 'for',\n",
       " 'everyday',\n",
       " 'life',\n",
       " 'im',\n",
       " 'andrew',\n",
       " 'huberman',\n",
       " 'and']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Because language models are computations done on numbers, we must convert any characters into values to use in these computations. We will stick to lowercase alphanumeric characters for this simple model, but the characterset may be as large as needed. We use a simple enumeration on the characters present, but more advanced models may use tokenizers and word embeddings.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#character encoding\n",
    "#chars = sorted(list(set(train_content).union(set(val_content))))\n",
    "\n",
    "#create mappings\n",
    "\n",
    "#character-level\n",
    "#chtoi = {ch : i for i, ch in enumerate(chars)}\n",
    "#itoch = {i : ch for i, ch in enumerate(chars)}\n",
    "\n",
    "#word_level\n",
    "wtoi = {w : i for i, w in enumerate(words)}\n",
    "itow = {i : w for w, i in wtoi.items()}\n",
    "\n",
    "\n",
    "#simple encodings\n",
    "encode = lambda chunk : [wtoi[word] for word in chunk]\n",
    "decode = lambda code : ' '.join([itow[i] for i in code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To get the tensors of encoded characters, we create the following functions that obtain random chunks from the data then draw batch size number of random samples from these chunks of length block size.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    #returns random chunk of data from split using word tokens\n",
    "    data = train_content if split == 'train' else val_content\n",
    "    size = train_len if split == 'train' else val_len\n",
    "    \n",
    "    start = random.randint(0, size - block * batch) #must start far enough back\n",
    "    \n",
    "    chunk = data[start : start + (block*batch)]\n",
    "    data = torch.tensor(encode(chunk), dtype=torch.long)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    #get random batches with expected outputs\n",
    "    chunk = get_random_chunk(split)\n",
    "    ix = torch.randint(len(chunk) - block, (batch,)) #tensor of random starting points\n",
    "    x = torch.stack([chunk[i : i+block] for i in ix]) #tensor of sequences\n",
    "    y = torch.stack([chunk[i+1 : i+block+1] for i in ix]) #tensor of sequences plus next char\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Now we move on to the transformer architecture. The head module is the basic building block of the multihead attention architecture, which we will discuss in more detail in a moment. The head computes attention weights between the input encodings and produces a context vector. \n",
    "\n",
    "To do this, we apply linear transformations to the data before masking it. Masking via torch.tril essentially prevents lookahead bias, so the model can only look at previous time steps but not future ones. Softmax then creates probabilities from the output that the model can then use for generating values.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''one head of self-attention'''\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block, block)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        weight = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        weight = weight.masked_fill(self.tril[: T, : T] == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = weight @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The most prevelant part of this architecture is the multihead attention module. The core idea of this is that each head is learning different representations of the input from different perspectives. These outputs are then concatenated and we set 20% of the inputs to 0 for regularization to introduce noise and make the model more robust via the nn.Dropout method.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attn in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Linear layer followed by nonlinearity\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(n_embd, 4 * n_embd),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(4 * n_embd, n_embd),\n",
    "                                nn.Dropout(dropout)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Within the block, we have our multihead attention function as well as a residual connection with a feedforward network. Essentially what this does is retain some of the data without transformations and add it to the transformed data. This allows for the model to \"remember\" some information as most information is actually lost in the transformations. We see this take place in the 'forward' method of the Block class.\n",
    "\n",
    "The feedforward method, as we see above, is a sequentially layered transformation that applies two linear functions outside of a ReLU function. For those who do not know, ReLU zeros out negative entries and retains positive ones. This is useful in introducing nonlinearity and mitigating the vanishing gradient problem. The vanishing gradient problem is an issue where in training the model, the sequence length increases, which prevents the weights from being updated effectively.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        #normalizations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GPTHuberman(nn.Module):\n",
    "    def __init__(self, charset_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_tbl = nn.Embedding(charset_size, n_embd)\n",
    "        self.pos_emb_tbl = nn.Embedding(block, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, charset_size)\n",
    "        \n",
    "        #initialize weights for each submodel\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        #index and target are (B, T) tensor of ints\n",
    "        tok_emb = self.token_emb_tbl(index)\n",
    "        pos_emb = self.pos_emb_tbl(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, new_tok):\n",
    "        for _ in range(new_tok):\n",
    "            index_crop = index[:, -block :]\n",
    "            #get predictions\n",
    "            logits, loss = self.forward(index_crop)\n",
    "            logits = logits[:, -1, :] #last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)#next token\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "            \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = GPTHuberman(len(words))\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def est_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model.forward(X, targets=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Train Loss: 10.273, Val Loss: 10.271\n",
      "Iteration: 100, Train Loss: 9.419, Val Loss: 9.391\n",
      "Iteration: 200, Train Loss: 7.915, Val Loss: 7.832\n",
      "Iteration: 300, Train Loss: 7.220, Val Loss: 7.116\n",
      "Iteration: 400, Train Loss: 6.861, Val Loss: 6.772\n",
      "Iteration: 500, Train Loss: 6.730, Val Loss: 6.597\n",
      "Iteration: 600, Train Loss: 6.652, Val Loss: 6.534\n",
      "Iteration: 700, Train Loss: 6.642, Val Loss: 6.496\n",
      "Iteration: 800, Train Loss: 6.610, Val Loss: 6.471\n",
      "Iteration: 900, Train Loss: 6.610, Val Loss: 6.417\n",
      "6.364140510559082\n"
     ]
    }
   ],
   "source": [
    "#optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        losses = est_loss()\n",
    "        print(f\"Iteration: {_iter}, Train Loss: {losses['train']:.3f}, Val Loss: {losses['val']:.3f}\")\n",
    "    \n",
    "    xb, yb = get_batch(train_content)\n",
    "    \n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the GPTHuberman Prompt. Enter 'STOP' to quit.\n",
      "Prompt: cell\n",
      "cell great ingesting with some of for off mechanism newborn to a pretty wakes actually old and who back next is the per makes the i\n",
      "Prompt: STOP\n"
     ]
    }
   ],
   "source": [
    "#prompt\n",
    "user_input = ''\n",
    "print(\"Welcome to the GPTHuberman Prompt. Enter 'STOP' to quit.\")\n",
    "while user_input != 'STOP':\n",
    "    user_input = input(\"Prompt: \")\n",
    "    \n",
    "    if user_input == 'STOP':\n",
    "        break\n",
    "        \n",
    "    data = [word.lower() for word in user_input.split()]\n",
    "    context = torch.tensor(encode(data), dtype=torch.long, device=device)\n",
    "    generated_chars = decode(m.generate(context.unsqueeze(0), new_tok=25)[0].tolist())\n",
    "    print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>So, what are some improvements that we can make on this model? For one, you will notice that if you do not enter a word that has been tokenized, the generate function throws an error. To remedy this, we could introduce a second model that tokenizes and trains at the character level. There are several options to choose from here and more research is required to determine the best method. Furthermore, we see that the sentences generated are gramatically incorrect. Improvement here requires establishing rules, assigning words to parts of the sentence (verb, noun, adjective), and training the model to follow these rules. More research is needed to determine how to add this to the algorithm.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "- [Huberman Podcast Transcript Data](https://www.kaggle.com/datasets/piyusharma/andrew-huberman-podcast-transcripts-95-episodes?resource=download)\n",
    "- [LLM Transformer Architecture Overview](https://www.youtube.com/watch?v=UU1WVnMk4E8)\n",
    "- [Base Model Repo](https://github.com/Infatoshi/fcc-intro-to-llms/blob/main/gpt-v1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
